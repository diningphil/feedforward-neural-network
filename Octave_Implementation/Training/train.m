 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %  @Input() %  X: input examples vector %  Y: input targets vector %  W: 1xn cell matrix, where n is #layers. Each cell contains an hidden layer weight matrix_type %  W_O: output weights matrix_type %  a: sigmoid constant %  eta: learning rate %  alpha: momentum constant %  lambda: regularization constant %  activationFunction %  activationGradient %  lossFunction %  estimateMeasure %  epochs: max #epochs %  threshold: error threshold % %  This function performs backpropagation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%function [TR_Err_best, TR_estimateMeasure_best, nIter_best, W_best, W_O_best, output_hidden_units_best] = train(X, Y, ...                                    eta, lambda, alpha, outputActivationType, hiddenActivationType, lossType, estimateMeasure, threshold, init_range, maxIter, layers_dims, runs, useAnnealing, useFanIn, showPlots)  addpath(genpath('../Estimates'));    % Choose the run which performs best  W_best = NaN;  W_O_best = NaN;  tr_best = inf;  TR_Err_best = NaN;  TR_estimateMeasure_best = NaN;  nIter_best = 0;  output_hidden_units_best = NaN;    ########## LOSS FUNCTION ####################  useLms = true;  if(strcmp(lossType, "lms") == 1)    #printf("LOSS: lms...\n");    lossFunction = @(O,Y)( leastMeanSquare(O,Y) );  elseif (strcmp(lossType,"crossentropy") == 1)    #printf("LOSS: cross entropy...\n");    lossFunction = @(O,Y) (crossEntropy(O,Y));    useLms = false;  else    printf("Invalid parameter, default loss: lms...\n");  endif  ########### OUTPUT ACTIVATION ################  [outputActivation,outputGradient] = chooseOutput(outputActivationType);  ########### HIDDEN ACTIVATION ################  [hiddenActivation,hiddenGradient] = chooseHidden(hiddenActivationType);  ##############################################     % No of training examples  m = size(X,1);    % Add column of ones to X;  X = [ones(m,1) X];    % No of input features   noOfFeats = size(X,2);    % Number of output units  out_dim = size(Y,2);    errors = [];  accuracies = [];    for r=1:runs    #printf("Run %d...\n", r);        [W, W_O] = initLayers(noOfFeats-1, layers_dims, out_dim, init_range, useFanIn); % n-1 to exclude additional 1s column      % Compute initial error    [O, nets_of_output_units, nets_of_hidden_units, outs_of_hidden_units] = feedforward(X, W, W_O, outputActivation, hiddenActivation);    Err_tot = lossFunction(O, Y);    TR_Err(1) = Err_tot;    A = estimateMeasure(O, Y);    TR_estimateMeasure(1) = A;    #output_hidden_units = outs_of_hidden_units;    nIter = 1;        ####### INIT PLOTS ######    if (showPlots)         set(0, 'defaultaxesfontsize', 18);                 subplot(2,1,1);      p1 = plot(1:nIter, TR_estimateMeasure(1:nIter), 'marker', "none");%,'linestyle', '-');            h = legend ("TR set");      title("Estimation per Epoch", "fontsize", 24);      xlabel("Epochs");      ylabel("Estimation");      subplot(2,1,2);      p2 = plot(1:nIter, TR_Err(1:nIter), 'marker', "none");%,'linestyle', '-');            h = legend ("TR set");            title("Error per Epoch", "fontsize", 24);      #printf("Training error on the whole dataset: %f \n", TR_Err(nIter));      xlabel("Epochs");      ylabel("Error");            drawnow()    endif     ##########################           % prepare the cell vector of old hidden units deltas for momentum implementation    D_old = {};    for w = W       D_old(1,end+1) = zeros(size(cell2mat(w)));    endfor;    D_O_old = zeros(size(W_O));    % backpropagation loop#    ((Err_tot) > threshold) &&     while ( (nIter < maxIter) )            % simple learning-rate annealing            if(useAnnealing)        eta_t = eta/(1 + nIter/m);      else        eta_t = eta;      endif      nIter = nIter + 1;      xAxis = 1:nIter;                  % Delicate step (do not reverse the order :] )      delta = (Y - O);               delta_out = delta;      if(useLms)        delta_out = (delta).* outputGradient(nets_of_output_units); % a row for each pattern       endif            % Compute weights adjustment for the least hidden layer and the output units      D_O_grad = (1/m)*(eta_t)*(delta_out'*cell2mat(outs_of_hidden_units(1,end)));      D_O_reg  = -2*lambda*W_O;      D_O_mom  = alpha*D_O_old;      D_O = D_O_grad + D_O_reg + D_O_mom;            D_O_old = D_O_grad + D_O_mom;               d = size(W,2); % extract the number of deltas (=hidden layers) to compute      W_O_nobias = W_O(:, 2:end);      upper_layer_nobias = W_O_nobias; % matrix of the next layer (initially W_O) without bias      upper_layer_delta = delta_out; % delta matrix of the upper layer (initially out)                  % Consider layers in reverse order      for n=fliplr(nets_of_hidden_units)                cur_layer_nets = cell2mat(n);                if(d > 1)           lower_layer_out = cell2mat(outs_of_hidden_units(1,d-1));              else % d == 1, consider the input matrix          lower_layer_out = X;        endif              delta_hidden = (upper_layer_nobias'*upper_layer_delta') .* hiddenGradient(cur_layer_nets');                %% temp variable to avoid calling cell2mat many times         W_d = cell2mat(W(1,d));                %% no regularization on bias (check with/wo)        D_d_grad = (1/m)*(eta_t)*(delta_hidden * lower_layer_out);        D_d_reg  = -2*lambda*W_d;        D_d_mom  = alpha*cell2mat(D_old(1,d));        D_d = D_d_grad + D_d_reg + D_d_mom;                D_old(1,d) = D_d_grad + D_d_mom;                upper_layer_nobias = W_d(:, 2:end);        upper_layer_delta = delta_hidden';        % Now update weights        % NOTE: this line MUST stay here, to avoid interference with next for-loop iterations        W(1,d) = W_d + D_d;        d = d-1;      endfor              % Update weights for the "rightmost" matrix      % NOTE: this line MUST stay here, to avoid interference of D_O during the algorithm      W_O += D_O;            % compute new output error      Err_tot = 0;      [O, nets_of_output_units, nets_of_hidden_units, outs_of_hidden_units] = feedforward(X, W, W_O, outputActivation, hiddenActivation);      %Err_tot = estimateError(O, Y);      Err_tot = lossFunction(O, Y);      A = estimateMeasure(O, Y);      TR_estimateMeasure(nIter) = A;      TR_Err(nIter) = Err_tot;       #output_hidden_units(nIter) = outs_of_hidden_units;                        ###### UPDATE PLOTS ######      if (showPlots)        set(p1, 'xdata', xAxis, 'ydata', TR_estimateMeasure);        set(p2, 'xdata', xAxis, 'ydata', TR_Err);                drawnow();      endif      ##########################          endwhile        errors = [errors; Err_tot];    accuracies = [accuracies; A];        if Err_tot < tr_best      W_best = W;      W_O_best = W_O;      tr_best = Err_tot;      TR_Err_best = TR_Err;      TR_estimateMeasure_best = TR_estimateMeasure;               nIter_best = nIter;      #output_hidden_units_best = outs_of_hidden_units;    endif  endfor  best_error_TR     = TR_Err_best(nIter_best)  best_accuracy_TR  = TR_estimateMeasure_best(nIter_best)  mean_error        = mean(errors)  std_error         = std(errors,1)  mean_acc          = mean(accuracies)  std_acc           = std(accuracies,1)end