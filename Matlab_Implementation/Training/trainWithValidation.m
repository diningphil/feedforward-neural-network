 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %  @Input() %  X: input examples vector %  Y: input targets vector %  Xval: input examples test vector %  Yval: input targets test vector %  W: 1xn cell matrix, where n is #layers. Each cell contains an hidden layer weight matrix_type %  W_O: output weights matrix_type %  a: sigmoid constant %  eta: learning rate %  alpha: momentum constant %  lambda: regularization constant %  activationFunction %  activationGradient %  lossFunction %  estimateMeasure %  epochs: max #epochs %  threshold: error threshold % %  This function performs backpropagation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% function [TR_Err_best, TR_estimateMeasure_best, nIter_best, W_best, W_O_best, output_hidden_units_best, TS_Err_best, TS_Acc_best] = trainWithValidation(X, Y, Xval, Yval, ...                                    eta, lambda, alpha, outputActivationType, hiddenActivationType, lossType, estimateMeasure, threshold, init_range, maxIter, layers_dims, runs, useAnnealing, useFanIn, showPlots)  addpath(genpath('../Estimates'));    % Choose the run which performs best  W_best = NaN;  W_O_best = NaN;  %tr_best = inf;  TR_Err_best = NaN;  TR_estimateMeasure_best = NaN;  nIter_best = 0;  output_hidden_units_best = NaN;    %## In this function I choose the best run depending on the result obtained on the validation set  ts_best = inf;    %########## LOSS FUNCTION ####################  useLms = true;  if(strcmp(lossType, 'lms') == 1)    %fprintf('LOSS: lms...\n');    lossFunction = @(O,Y)( leastMeanSquare(O,Y) );  elseif (strcmp(lossType,'crossentropy') == 1)    %fprintf('LOSS: cross entropy...\n');    lossFunction = @(O,Y) (crossEntropy(O,Y));    useLms = false;  else    fprintf('Invalid parameter, default loss: lms...\n');  end  %########### OUTPUT ACTIVATION ################  [outputActivation,outputGradient] = chooseOutput(outputActivationType);  %########### HIDDEN ACTIVATION ################  [hiddenActivation,hiddenGradient] = chooseHidden(hiddenActivationType);  %##############################################    % No of training examples  m = size(X,1);    % Add column of ones to X;  X = [ones(m,1) X];  Xval = [ones(size(Xval,1),1) Xval];    % No of input features   noOfFeats = size(X,2);    % Number of output units  out_dim = size(Y,2);    errors = [];  accuracies = [];    fprintf('##### SIMPLE TRAINING WITH TEST BEGINS #####\n\n');  fprintf('##### (Eta=%f, Alpha=%f, Lambda=%f) BEGINS #####\n', eta, alpha, lambda);      for r=1:runs    %fprintf('Run %d...\n', r);        [W, W_O] = initLayers(noOfFeats-1, layers_dims, out_dim, init_range, true); % n-1 to exclude additional 1s column      % Compute initial error    [O, nets_of_output_units, nets_of_hidden_units, outs_of_hidden_units] = feedforward(X, W, W_O, outputActivation, hiddenActivation);    Err_tot = lossFunction(O, Y);    TR_Err(1) = Err_tot;        A = estimateMeasure(O, Y);    TR_estimateMeasure(1) = A;        %output_hidden_units(1) = outs_of_hidden_units;    nIter = 1;    % Initial test error    Otest = feedforward(Xval, W, W_O, outputActivation, hiddenActivation);    TS_Err(1) = lossFunction(Otest, Yval);    TS_Acc(1) = estimateMeasure(Otest, Yval);        %####### INIT PLOTS ######    if (showPlots)         set(0, 'defaultaxesfontsize', 18);                 subplot(2,2,1);      p1 = plot(1:nIter, TR_estimateMeasure(1:nIter), 'marker', 'none', 'color', 'red');%,'linestyle', '-');      hold on;      p22 = plot(1:nIter, TS_Acc(1:nIter), 'marker', 'none', 'linestyle', '--');      hold off;            h = legend ('TR set', 'TS set');      title('Estimation per Epoch', 'fontsize', 24);      xlabel('Epochs');      ylabel('MEE');      subplot(2,2,2);      p2 = plot(1:nIter, TR_Err(1:nIter), 'marker', 'none', 'color', 'red');%,'linestyle', '-');            % for TS set      hold on;      p21 = plot(1:nIter, TS_Err(1:nIter), 'marker', 'none', 'linestyle', '--');      hold off;      h = legend ('TR set', 'TS set');               title('Error per Epoch', 'fontsize', 24);      %fprintf('Training error on the whole dataset: %f \n', TR_Err(nIter));      xlabel('Epochs');      ylabel('MSE');            subplot(2,2,3);      p3 = plot(200, 0, 'marker', 'none', 'color', 'red');%,'linestyle', '-');      hold on;      p4 = plot(200, 0, 'marker', 'none', 'linestyle', '--');      hold off;            subplot(2,2,4);      p5 = plot(200, 0, 'marker', 'none', 'color', 'red');%,'linestyle', '-');      hold on;      p6 = plot(200, 0, 'marker', 'none', 'linestyle', '--');      hold off;            drawnow()    end     %##########################           % prepare the cell vector of old hidden units deltas for momentum implementation    D_old = {};    for w = W       D_old{1,end+1} = zeros(size(cell2mat(w)));    end    D_O_old = zeros(size(W_O));    % backpropagation loop    while ( ((Err_tot) > threshold) && (nIter < maxIter) )            % simple learning-rate annealing      if(useAnnealing)        eta_t = eta/(1 + nIter/m);      else        eta_t = eta;      end            nIter = nIter + 1;      xAxis = 1:nIter;                  % Delicate step (do not reverse the order :] )      delta = (Y - O);               delta_out = delta;      if(useLms)        delta_out = (delta).* outputGradient(nets_of_output_units); % a row for each pattern       end            % Compute weights adjustment for the least hidden layer and the output units      D_O_grad = (1/m)*(eta_t)*(delta_out'*cell2mat(outs_of_hidden_units(1,end)));      D_O_reg  = -2*lambda*W_O;      D_O_mom  = alpha*D_O_old;      D_O = D_O_grad + D_O_reg + D_O_mom;            D_O_old = D_O_grad + D_O_mom;               d = size(W,2); % extract the number of deltas (=hidden layers) to compute              W_O_nobias = W_O(:, 2:end);      upper_layer_nobias = W_O_nobias; % matrix of the next layer (initially W_O) without bias      upper_layer_delta = delta_out; % delta matrix of the upper layer (initially out)                  % Consider layers in reverse order      for n=fliplr(nets_of_hidden_units)                cur_layer_nets = cell2mat(n);                if(d > 1)           lower_layer_out = cell2mat(outs_of_hidden_units(1,d-1));              else % d == 1, consider the input matrix          lower_layer_out = X;        end              delta_hidden = (upper_layer_nobias'*upper_layer_delta') .* hiddenGradient(cur_layer_nets');                % temp variable to avoid calling cell2mat many times         W_d = cell2mat(W(1,d));                % no regularization on bias (check with/wo)        D_d_grad = (1/m)*(eta_t)*(delta_hidden * lower_layer_out);        D_d_reg  = -2*lambda*W_d;        D_d_mom  = alpha*cell2mat(D_old(1,d));        D_d = D_d_grad + D_d_reg + D_d_mom;                D_old{1,d} = D_d_grad + D_d_mom;                upper_layer_nobias = W_d(:, 2:end);        upper_layer_delta = delta_hidden';        % Now update weights        % NOTE: this line MUST stay here, to avoid interference with next for-loop iterations        W{1,d} = W_d + D_d;        d = d-1;      end              % Update weights for the 'rightmost' matrix      % NOTE: this line MUST stay here, to avoid interference of D_O during the algorithm      W_O = W_O + D_O;            % compute new output error      Err_tot = 0;      [O, nets_of_output_units, nets_of_hidden_units, outs_of_hidden_units] = feedforward(X, W, W_O, outputActivation, hiddenActivation);      %Err_tot = estimateError(O, Y);      Err_tot = lossFunction(O, Y);      A = estimateMeasure(O, Y);      TR_estimateMeasure(nIter) = A;      TR_Err(nIter) = Err_tot;       %output_hidden_units(nIter) = outs_of_hidden_units;            % Compute test error for this iteration #      Otest = feedforward(Xval, W, W_O, outputActivation, hiddenActivation);      E_ts = lossFunction(Otest, Yval);      TS_Err(nIter) = E_ts;      A_ts = estimateMeasure(Otest,Yval);      TS_Acc(nIter) = A_ts;                        %###### UPDATE PLOTS ######      if (showPlots)        if (nIter <= 200)          set(p1, 'xdata', xAxis, 'ydata', TR_estimateMeasure);          set(p2, 'xdata', xAxis, 'ydata', TR_Err);                    % For TS set          set(p21, 'xdata', xAxis, 'ydata', TS_Err);          set(p22, 'xdata', xAxis, 'ydata', TS_Acc);        end                if (nIter >= 200)          set(p3, 'xdata', 200:nIter, 'ydata', TR_estimateMeasure(200:nIter));          set(p4, 'xdata', 200:nIter, 'ydata', TS_Acc(200:nIter));          set(p5, 'xdata', 200:nIter, 'ydata', TR_Err(200:nIter));          set(p6, 'xdata', 200:nIter, 'ydata', TS_Err(200:nIter));        end        drawnow();      end      %##########################          end        % Collect Validation error and accuracy    errors = [errors; E_ts];    accuracies = [accuracies; A_ts];            %### PICK THE ONE THAT PERFORMS BEST ON VALIDATION SET! ( SINCE I HAVE IT )    if A_ts < ts_best      W_best = W;      W_O_best = W_O;      %tr_best = Err_tot;      TR_Err_best = TR_Err;      TR_estimateMeasure_best = TR_estimateMeasure;               TS_Err_best = TS_Err;      TS_Acc_best = TS_Acc;      ts_best = A_ts            nIter_best = nIter;      %output_hidden_units_best = outs_of_hidden_units;    end  end  useFanIn  useAnnealing  layers_dims  init_range  hiddenActivation  maxIter  runs  lossFunction    best_error_TR     = TR_Err_best(nIter_best)  best_accuracy_TR  = TR_estimateMeasure_best(nIter_best)  best_error_TS     = TS_Err_best(nIter_best)  best_accuracy_TS  = TS_Acc_best(nIter_best)  mean_error_TS        = mean(errors)  std_error_TS         = std(errors,1)  mean_acc_TS          = mean(accuracies)  std_acc_TS           = std(accuracies,1)    fprintf('##### (Eta=%f, Alpha=%f, Lambda=%f) ENDS #####\n\n', eta, alpha, lambda);  fprintf('##### SIMPLE TRAINING WITH TEST ENDS #####\n');end